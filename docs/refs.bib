
@article{holste_towards_2024,
	title = {Towards long-tailed, multi-label disease classification from chest {X}-ray: {Overview} of the {CXR}-{LT} challenge},
	volume = {97},
	issn = {1361-8415},
	shorttitle = {Towards long-tailed, multi-label disease classification from chest {X}-ray},
	url = {https://www.sciencedirect.com/science/article/pii/S136184152400149X},
	doi = {10.1016/j.media.2024.103224},
	abstract = {Many real-world image recognition problems, such as diagnostic medical imaging exams, are “long-tailed” – there are a few common findings followed by many more relatively rare conditions. In chest radiography, diagnosis is both a long-tailed and multi-label problem, as patients often present with multiple findings simultaneously. While researchers have begun to study the problem of long-tailed learning in medical image recognition, few have studied the interaction of label imbalance and label co-occurrence posed by long-tailed, multi-label disease classification. To engage with the research community on this emerging topic, we conducted an open challenge, CXR-LT, on long-tailed, multi-label thorax disease classification from chest X-rays (CXRs). We publicly release a large-scale benchmark dataset of over 350,000 CXRs, each labeled with at least one of 26 clinical findings following a long-tailed distribution. We synthesize common themes of top-performing solutions, providing practical recommendations for long-tailed, multi-label medical image classification. Finally, we use these insights to propose a path forward involving vision-language foundation models for few- and zero-shot disease classification.},
	urldate = {2025-01-30},
	journal = {Medical Image Analysis},
	author = {Holste, Gregory and Zhou, Yiliang and Wang, Song and Jaiswal, Ajay and Lin, Mingquan and Zhuge, Sherry and Yang, Yuzhe and Kim, Dongkyun and Nguyen-Mau, Trong-Hieu and Tran, Minh-Triet and Jeong, Jaehyup and Park, Wongi and Ryu, Jongbin and Hong, Feng and Verma, Arsh and Yamagishi, Yosuke and Kim, Changhyun and Seo, Hyeryeong and Kang, Myungjoo and Celi, Leo Anthony and Lu, Zhiyong and Summers, Ronald M. and Shih, George and Wang, Zhangyang and Peng, Yifan},
	month = oct,
	year = {2024},
	keywords = {Chest X-ray, Computer-aided diagnosis, Long-tailed learning},
	pages = {103224},
	file = {ScienceDirect Snapshot:C\:\\Users\\rshdk\\Zotero\\storage\\DPAM67CD\\S136184152400149X.html:text/html;Submitted Version:C\:\\Users\\rshdk\\Zotero\\storage\\X8PTHQ34\\Holste et al. - 2024 - Towards long-tailed, multi-label disease classification from chest X-ray Overview of the CXR-LT cha.pdf:application/pdf},
}

@misc{michalopoulos_umlsbert_2021,
	title = {{UmlsBERT}: {Clinical} {Domain} {Knowledge} {Augmentation} of {Contextual} {Embeddings} {Using} the {Unified} {Medical} {Language} {System} {Metathesaurus}},
	shorttitle = {{UmlsBERT}},
	url = {http://arxiv.org/abs/2010.10391},
	doi = {10.48550/arXiv.2010.10391},
	abstract = {Contextual word embedding models, such as BioBERT and Bio\_ClinicalBERT, have achieved state-of-the-art results in biomedical natural language processing tasks by focusing their pre-training process on domain-specific corpora. However, such models do not take into consideration expert domain knowledge. In this work, we introduced UmlsBERT, a contextual embedding model that integrates domain knowledge during the pre-training process via a novel knowledge augmentation strategy. More specifically, the augmentation on UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus was performed in two ways: i) connecting words that have the same underlying `concept' in UMLS, and ii) leveraging semantic group knowledge in UMLS to create clinically meaningful input embeddings. By applying these two strategies, UmlsBERT can encode clinical domain knowledge into word embeddings and outperform existing domain-specific models on common named-entity recognition (NER) and clinical natural language inference clinical NLP tasks.},
	urldate = {2025-06-01},
	publisher = {arXiv},
	author = {Michalopoulos, George and Wang, Yuanxin and Kaka, Hussam and Chen, Helen and Wong, Alexander},
	month = jun,
	year = {2021},
	note = {arXiv:2010.10391 [cs]
version: 5},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 10 pages, 3 figures, accepted in NAACL 2021},
	file = {Preprint PDF:C\:\\Users\\rshdk\\Zotero\\storage\\UAPDW2QG\\Michalopoulos et al. - 2021 - UmlsBERT Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical.pdf:application/pdf;Snapshot:C\:\\Users\\rshdk\\Zotero\\storage\\NJM55MWB\\2010.html:text/html},
}

@misc{johnson_mimic-cxr-jpg_nodate,
	title = {{MIMIC}-{CXR}-{JPG} - chest radiographs with structured labels},
	url = {https://physionet.org/content/mimic-cxr-jpg/2.0.0/},
	doi = {10.13026/8360-T248},
	abstract = {The MIMIC Chest X-ray JPG (MIMIC-CXR-JPG) Database v2.0.0 is a large publicly
available dataset of chest radiographs in JPG format with structured labels
derived from free-text radiology reports. The MIMIC-CXR-JPG dataset is wholly
derived from [MIMIC-CXR](https://physionet.org/content/mimic-cxr/), providing
JPG format files derived from the DICOM images and structured labels derived
from the free-text reports. The aim of MIMIC-CXR-JPG is to provide a
convenient processed version of MIMIC-CXR, as well as to provide a standard
reference for data splits and image labels. The dataset contains 377,110 JPG
format images and structured labels derived from the 227,827 free-text
radiology reports associated with these images. The dataset is de-identified
to satisfy the US Health Insurance Portability and Accountability Act of 1996
(HIPAA) Safe Harbor requirements. Protected health information (PHI) has been
removed. The dataset is intended to support a wide body of research in
medicine including image understanding, natural language processing, and
decision support.},
	urldate = {2025-06-01},
	publisher = {PhysioNet},
	author = {Johnson, Alistair and Lungren, Matt and Peng, Yifan and Lu, Zhiyong and Mark, Roger and Berkowitz, Seth and Horng, Steven},
}

@misc{holste_cxr-lt_nodate,
	title = {{CXR}-{LT}: {Multi}-{Label} {Long}-{Tailed} {Classification} on {Chest} {X}-{Rays}},
	shorttitle = {{CXR}-{LT}},
	url = {https://physionet.org/content/cxr-lt-iccv-workshop-cvamd/2.0.0/},
	doi = {10.13026/RYJ9-X506},
	abstract = {Chest radiography presents a "long-tailed" distribution of findings, where a
few diseases are common, but most are rare. Diagnosis is further complicated
by its multi-label nature, as patients often exhibit multiple co-occurring
findings. While recent research has attempted to address the long-tailed
medical image classification problem, the interplay between class imbalance
and label co-occurrence remains underexplored. The **CXR-LT 2024 challenge**
builds on the success of CXR-LT 2023, expanding the dataset of 377,110 chest
X-rays (CXRs) to 45 disease labels, including 19 new rare disease findings.
This year 's challenge introduces three tasks: (i) long-tailed classification
on a large, noisy test set, (ii) long-tailed classification on a manually
annotated "gold standard" subset, and (iii) zero-shot generalization to five
previously unseen disease findings. CXR-LT 2024 addresses critical challenges
in long-tailed, multi-label, and zero-shot learning for medical imaging by
synthesizing state-of-the-art solutions from the international research
community. Further, our dataset contributions -- expanding disease coverage to
better reflect real-world clinical settings -- offer a valuable resource for
future research. This project contains labels from the CXR-LT 2024 and CXR-LT
2023 challenges, as well as a related subset used in the MICCAI 2023 paper,
"How Does Pruning Impact Multi-Label Long-Tailed Learning?"},
	urldate = {2025-06-01},
	publisher = {PhysioNet},
	author = {Holste, Gregory and Lin, Mingquan and Wang, Song and Zhou, Yiliang and Wei, Yishu and Chen, Hao and Wang, Atlas and Peng, Yifan},
}

@article{gu_domain-specific_2022,
	title = {Domain-{Specific} {Language} {Model} {Pretraining} for {Biomedical} {Natural} {Language} {Processing}},
	volume = {3},
	issn = {2691-1957, 2637-8051},
	url = {http://arxiv.org/abs/2007.15779},
	doi = {10.1145/3458754},
	abstract = {Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this paper, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly-available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition (NER). To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding \& Reasoning Benchmark) at https://aka.ms/BLURB.},
	number = {1},
	urldate = {2025-06-02},
	journal = {ACM Trans. Comput. Healthcare},
	author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
	month = jan,
	year = {2022},
	note = {arXiv:2007.15779 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {1--23},
	annote = {Comment: ACM Transactions on Computing for Healthcare (HEALTH)},
	file = {Preprint PDF:C\:\\Users\\rshdk\\Zotero\\storage\\V9AB2ZK5\\Gu et al. - 2022 - Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing.pdf:application/pdf;Snapshot:C\:\\Users\\rshdk\\Zotero\\storage\\3HEVBBJ9\\2007.html:text/html},
}

@misc{park_robust_2023,
	title = {Robust {Asymmetric} {Loss} for {Multi}-{Label} {Long}-{Tailed} {Learning}},
	url = {http://arxiv.org/abs/2308.05542},
	doi = {10.48550/arXiv.2308.05542},
	abstract = {In real medical data, training samples typically show long-tailed distributions with multiple labels. Class distribution of the medical data has a long-tailed shape, in which the incidence of different diseases is quite varied, and at the same time, it is not unusual for images taken from symptomatic patients to be multi-label diseases. Therefore, in this paper, we concurrently address these two issues by putting forth a robust asymmetric loss on the polynomial function. Since our loss tackles both long-tailed and multi-label classification problems simultaneously, it leads to a complex design of the loss function with a large number of hyper-parameters. Although a model can be highly fine-tuned due to a large number of hyper-parameters, it is difficult to optimize all hyper-parameters at the same time, and there might be a risk of overfitting a model. Therefore, we regularize the loss function using the Hill loss approach, which is beneficial to be less sensitive against the numerous hyper-parameters so that it reduces the risk of overfitting the model. For this reason, the proposed loss is a generic method that can be applied to most medical image classification tasks and does not make the training process more time-consuming. We demonstrate that the proposed robust asymmetric loss performs favorably against the long-tailed with multi-label medical image classification in addition to the various long-tailed single-label datasets. Notably, our method achieves Top-5 results on the CXR-LT dataset of the ICCV CVAMD 2023 competition. We opensource our implementation of the robust asymmetric loss in the public repository: https://github.com/kalelpark/RAL.},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Park, Wongi and Park, Inhyuk and Kim, Sungeun and Ryu, Jongbin},
	month = aug,
	year = {2023},
	note = {arXiv:2308.05542 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rshdk\\Zotero\\storage\\9AW8Z6BZ\\Park et al. - 2023 - Robust Asymmetric Loss for Multi-Label Long-Tailed Learning.pdf:application/pdf;Snapshot:C\:\\Users\\rshdk\\Zotero\\storage\\CGTJIQZB\\2308.html:text/html},
}

@misc{kim_chexfusion_2023,
	title = {{CheXFusion}: {Effective} {Fusion} of {Multi}-{View} {Features} using {Transformers} for {Long}-{Tailed} {Chest} {X}-{Ray} {Classification}},
	shorttitle = {{CheXFusion}},
	url = {http://arxiv.org/abs/2308.03968},
	doi = {10.48550/arXiv.2308.03968},
	abstract = {Medical image classification poses unique challenges due to the long-tailed distribution of diseases, the co-occurrence of diagnostic findings, and the multiple views available for each study or patient. This paper introduces our solution to the ICCV CVAMD 2023 Shared Task on CXR-LT: Multi-Label Long-Tailed Classification on Chest X-Rays. Our approach introduces CheXFusion, a transformer-based fusion module incorporating multi-view images. The fusion module, guided by self-attention and cross-attention mechanisms, efficiently aggregates multi-view features while considering label co-occurrence. Furthermore, we explore data balancing and self-training methods to optimize the model's performance. Our solution achieves state-of-the-art results with 0.372 mAP in the MIMIC-CXR test set, securing 1st place in the competition. Our success in the task underscores the significance of considering multi-view settings, class imbalance, and label co-occurrence in medical image classification. Public code is available at https://github.com/dongkyuk/CXR-LT-public-solution},
	urldate = {2025-06-02},
	publisher = {arXiv},
	author = {Kim, Dongkyun},
	month = aug,
	year = {2023},
	note = {arXiv:2308.03968 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\rshdk\\Zotero\\storage\\FFJZB3VF\\Kim - 2023 - CheXFusion Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray C.pdf:application/pdf;Snapshot:C\:\\Users\\rshdk\\Zotero\\storage\\KLM27BCI\\2308.html:text/html},
}
